{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open('job_intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?' , '!' , '.' , ',' , ' ? ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Hey'], 'greeting'), (['Ekse'], 'greeting'), (['Hi'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Ok', 'bye'], 'goodbye'), (['Bye', 'Bye'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['What', 'is', 'your', 'name'], 'name'), (['Whats', 'your', 'name'], 'name'), (['Tell', 'me', 'your', 'name'], 'name'), (['Who', 'are', 'you'], 'name'), (['Ungubani'], 'name'), (['Tell', 'me', 'about', 'yourself'], 'name'), (['You', 'are'], 'name'), (['What', 'do', 'you', 'do'], 'options'), (['How', 'can', 'you', 'help'], 'options'), (['What', 'do', 'you', 'know'], 'options'), (['What', 'is', 'your', 'purpose'], 'options'), (['How', 'can', 'you', 'help'], 'options'), (['South', 'Africa'], 'south_africa_info'), (['What', 'can', 'you', 'tell', 'me', 'about', 'SA'], 'south_africa_info'), (['SA'], 'south_africa_info'), (['Tell', 'me', 'about', 'SA'], 'south_africa_info'), (['What', 'about', 'SA'], 'south_africa_info'), (['What', 'do', 'you', 'know', 'about', 'SA'], 'south_africa_info'), (['Tell', 'me', 'more', 'about', 'SA'], 'south_africa_info'), (['What', 'can', 'you', 'tell', 'me', 'about', 'South', 'Africa'], 'south_africa_info'), (['Tell', 'me', 'about', 'South', 'Africa'], 'south_africa_info'), (['What', 'about', 'South', 'Africa'], 'south_africa_info'), (['What', 'do', 'you', 'know', 'about', 'South', 'Africa'], 'south_africa_info'), (['Tell', 'me', 'more', 'about', 'South', 'Africa'], 'south_africa_info'), (['South', 'Africa', 'Facts'], 'south_africa_facts'), (['SA', 'Facts'], 'south_africa_facts'), (['Give', 'me', 'some', 'facts', 'about', 'SA'], 'south_africa_facts'), (['Give', 'me', 'some', 'facts', 'about', 'South', 'Africa'], 'south_africa_facts'), (['SA', 'Facts'], 'south_africa_facts'), (['SA', 'interesting', 'facts'], 'south_africa_facts'), (['South', 'Africa', 'facts'], 'south_africa_facts'), (['South', 'Africa', 'interesting', 'facts'], 'south_africa_facts'), (['Tell', 'me', 'something', 'interesting'], 'south_africa_facts')]\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list , intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "            \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(i) for i in words if i not in ignore_letters]\n",
    "words = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'Africa',\n",
       " 'Awesome',\n",
       " 'Bye',\n",
       " 'Ekse',\n",
       " 'Facts',\n",
       " 'Give',\n",
       " 'Good',\n",
       " 'Goodbye',\n",
       " 'Hello',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'Hola',\n",
       " 'How',\n",
       " 'Is',\n",
       " 'Ok',\n",
       " 'SA',\n",
       " 'See',\n",
       " 'South',\n",
       " 'Tell',\n",
       " 'Thank',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'Ungubani',\n",
       " 'What',\n",
       " 'Whats',\n",
       " 'Who',\n",
       " 'You',\n",
       " 'about',\n",
       " 'anyone',\n",
       " 'are',\n",
       " 'bye',\n",
       " 'can',\n",
       " 'day',\n",
       " 'do',\n",
       " 'fact',\n",
       " 'for',\n",
       " 'help',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'interesting',\n",
       " 'is',\n",
       " 'know',\n",
       " 'later',\n",
       " 'me',\n",
       " 'more',\n",
       " 'name',\n",
       " 'purpose',\n",
       " 'some',\n",
       " 'something',\n",
       " 'tell',\n",
       " 'thanks',\n",
       " 'there',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goodbye',\n",
       " 'greeting',\n",
       " 'name',\n",
       " 'options',\n",
       " 'south_africa_facts',\n",
       " 'south_africa_info',\n",
       " 'thanks']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words , open('words.pkl' , 'wb'))\n",
    "pickle.dump(classes , open('classes.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0]*len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        if word in word_patterns:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag , output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = [[[i,j] for j in range(1,3)] for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64 , input_shape = (len(train_x[0]) , ) ,activation = 'relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32 , activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]) , activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr = 0.01 , decay = 1e-6 , momentum=0.9 , nesterov=True)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = sgd , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53 samples\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 29ms/sample - loss: 1.9473 - accuracy: 0.2075\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 468us/sample - loss: 1.9460 - accuracy: 0.1509\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 434us/sample - loss: 1.9528 - accuracy: 0.1698\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 443us/sample - loss: 1.9387 - accuracy: 0.1509\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 418us/sample - loss: 1.8910 - accuracy: 0.1887\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 412us/sample - loss: 1.9204 - accuracy: 0.1698\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 367us/sample - loss: 1.8852 - accuracy: 0.2075\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 470us/sample - loss: 1.9382 - accuracy: 0.1698\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 413us/sample - loss: 1.9064 - accuracy: 0.1887\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 478us/sample - loss: 1.8428 - accuracy: 0.2453\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 411us/sample - loss: 1.8309 - accuracy: 0.2453\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 427us/sample - loss: 1.8117 - accuracy: 0.1887\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 490us/sample - loss: 1.8105 - accuracy: 0.3396\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 814us/sample - loss: 1.8412 - accuracy: 0.2075\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 445us/sample - loss: 1.8171 - accuracy: 0.2830\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 305us/sample - loss: 1.7832 - accuracy: 0.2264\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 271us/sample - loss: 1.7697 - accuracy: 0.2642\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 242us/sample - loss: 1.7519 - accuracy: 0.2830\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 246us/sample - loss: 1.8055 - accuracy: 0.1698\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 213us/sample - loss: 1.6963 - accuracy: 0.2830\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 207us/sample - loss: 1.7223 - accuracy: 0.2830\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 216us/sample - loss: 1.7905 - accuracy: 0.2453\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 183us/sample - loss: 1.7405 - accuracy: 0.2642\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 181us/sample - loss: 1.6666 - accuracy: 0.3585\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 188us/sample - loss: 1.7723 - accuracy: 0.2830\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 187us/sample - loss: 1.6517 - accuracy: 0.3019\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 159us/sample - loss: 1.6868 - accuracy: 0.3396\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 223us/sample - loss: 1.6996 - accuracy: 0.3396\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 201us/sample - loss: 1.6284 - accuracy: 0.3774\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 143us/sample - loss: 1.7090 - accuracy: 0.2830\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 181us/sample - loss: 1.6421 - accuracy: 0.4717\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 190us/sample - loss: 1.5764 - accuracy: 0.5283\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 154us/sample - loss: 1.5993 - accuracy: 0.3962\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 186us/sample - loss: 1.6074 - accuracy: 0.4151\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 184us/sample - loss: 1.5652 - accuracy: 0.3962\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 171us/sample - loss: 1.6001 - accuracy: 0.4340\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 185us/sample - loss: 1.5878 - accuracy: 0.3396\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 202us/sample - loss: 1.5931 - accuracy: 0.4528\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 193us/sample - loss: 1.6114 - accuracy: 0.3962\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 181us/sample - loss: 1.5590 - accuracy: 0.4528\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 193us/sample - loss: 1.4794 - accuracy: 0.4906\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 203us/sample - loss: 1.4895 - accuracy: 0.4906\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 215us/sample - loss: 1.4803 - accuracy: 0.4528\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 193us/sample - loss: 1.4974 - accuracy: 0.3962\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 201us/sample - loss: 1.5549 - accuracy: 0.4717\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 180us/sample - loss: 1.4495 - accuracy: 0.5094\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 175us/sample - loss: 1.4322 - accuracy: 0.4906\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 199us/sample - loss: 1.4511 - accuracy: 0.4340\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 218us/sample - loss: 1.3620 - accuracy: 0.5472\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 168us/sample - loss: 1.5179 - accuracy: 0.3585\n",
      "WARNING:tensorflow:From C:\\Users\\rohan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: rohan_model.model\\assets\n",
      "Done........!!\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(train_x) , np.array(train_y) , epochs=50)\n",
    "model.save('rohan_model.model')\n",
    "print('Done........!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_x)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
